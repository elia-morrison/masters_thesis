{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from hr_research.config import output_path\n",
    "from hr_research.models.seniority import RobertaForRegression\n",
    "from os.path import join\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = join(output_path, \"seniority_model_out/checkpoint-29000\")\n",
    "\n",
    "training_path = join(output_path, \"seniority_model_v2/\")\n",
    "training_logs = join(training_path, \"logs/\")\n",
    "dataset_path = join(output_path, \"seniority_pairs.jsonl\")\n",
    "reallife_dataset_path = join(output_path, \"reallife_seniority_pairs.jsonl\")\n",
    "SEQ_MAX_LEN = 64\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generator = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobTitleDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        with jsonlines.open(self.file_path, 'r') as reader:\n",
    "            data = [obj for obj in reader]\n",
    "        return data\n",
    "\n",
    "    def tokenize_data(self, junior_title, senior_title):\n",
    "        tokenized_pair = self.tokenizer(text=(junior_title, senior_title), truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        return tokenized_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        junior_title = random.choice(item['junior']).title()\n",
    "        senior_title = random.choice(item['senior']).title()\n",
    "        tokenized_pair = self.tokenize_data(junior_title, senior_title)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokenized_pair['input_ids']),\n",
    "            'attention_mask': torch.tensor(tokenized_pair['attention_mask']),\n",
    "            'labels': torch.ones(2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 0.01\n",
    "\n",
    "class PairwiseTrainer(Trainer):\n",
    "    def get_logits(self, model, inputs):\n",
    "        input_junior = {\n",
    "            'input_ids': inputs['input_ids'][:, 0, :].squeeze(dim=1),\n",
    "            'attention_mask': inputs['attention_mask'][:, 0, :].squeeze(dim=1),\n",
    "        }\n",
    "\n",
    "        input_senior = {\n",
    "            'input_ids': inputs['input_ids'][:, 1, :].squeeze(dim=1),\n",
    "            'attention_mask': inputs['attention_mask'][:, 1, :].squeeze(dim=1),\n",
    "        }\n",
    "        \n",
    "        output_junior = model(**input_junior)\n",
    "        output_senior = model(**input_senior)\n",
    "\n",
    "        return output_junior, output_senior\n",
    "    \n",
    "    def loss_from_logits(self, model, output_junior, output_senior):\n",
    "        diff = MARGIN + output_junior - output_senior\n",
    "        seniority_loss = torch.where(diff >= 0, \n",
    "                                     model.distance_loss(output_junior, torch.zeros_like(output_junior)) \n",
    "                                     + model.distance_loss(output_senior, torch.ones_like(output_senior)),\n",
    "                                     diff * 0)\n",
    "\n",
    "        # seniority_loss = torch.where(diff >= 0, \n",
    "        #                              #model.distance_loss(diff, -torch.ones_like(diff)),\n",
    "        #                              model.distance_loss(output_junior, output_junior + 1) \n",
    "        #                              + model.distance_loss(output_senior, output_senior - 1),\n",
    "        #                              diff * 0)\n",
    "\n",
    "        # seniority_loss = torch.mean(torch.log1p(torch.exp(3*diff - 0.8)))\n",
    "\n",
    "        # similarity_penalizer = 0.8 * torch.exp(-torch.pow(20*diff, 2))\n",
    "        # distance_penalizer = torch.log1p(torch.exp(3*diff - 1))\n",
    "\n",
    "        # seniority_loss = torch.where(diff >= 0,\n",
    "        #                              similarity_penalizer + distance_penalizer,\n",
    "        #                              0)\n",
    "\n",
    "        # min_jr = torch.min(torch.min(output_junior), torch.min(output_senior))\n",
    "        # max_sr = 1 - torch.max(torch.max(output_junior), torch.max(output_senior))\n",
    "        return torch.mean(seniority_loss)# + min_jr + max_sr\n",
    "\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        output_junior, output_senior = self.get_logits(model, inputs)\n",
    "        \n",
    "        return self.loss_from_logits(model, output_junior, output_senior)\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        with torch.no_grad():\n",
    "            output_junior, output_senior = self.get_logits(model, inputs)\n",
    "            loss = self.loss_from_logits(model, output_junior, output_senior)\n",
    "            logits = torch.stack((output_junior, output_senior), dim=1).detach()\n",
    "\n",
    "            # return loss and other outputs for evaluation\n",
    "            return (loss, logits, torch.ones_like(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_seniority_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, _ = eval_preds # labels are fake\n",
    "    logits = logits.squeeze()\n",
    "    correct_preds = 1. * (logits[:, 0] < logits[:, 1]) # 1. * converts to float\n",
    "    incorrect_indexes = np.where(logits[:, 0] >= logits[:, 1])\n",
    "\n",
    "    with open(\"incorrect_predictions.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"indices\": incorrect_indexes[0].tolist(),\n",
    "            \"values\": logits[incorrect_indexes].tolist()\n",
    "        }, f)\n",
    "    print(f\"INCORRECT: {incorrect_indexes}\")\n",
    "    return metric.compute(predictions=correct_preds, references=np.ones_like(correct_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JobTitleDataset(dataset_path, tokenizer, max_length=SEQ_MAX_LEN)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reallife_set = JobTitleDataset(reallife_dataset_path, tokenizer, max_length=SEQ_MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = random_split(dataset, [0.95, 0.05], generator=random_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForRegression.from_pretrained(checkpoint_path)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=training_path,          # output directory\n",
    "    num_train_epochs=40,              # total number of training epochs\n",
    "    per_device_train_batch_size=12,  # batch size per device during training\n",
    "    per_device_eval_batch_size=24,   # batch size for evaluation\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=training_logs,       # directory for storing logs\n",
    "    report_to='tensorboard',\n",
    ")\n",
    "\n",
    "trainer = PairwiseTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_seniority_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"incorrect_predictions.json\", \"r\") as f:\n",
    "    incorrect_preds = json.load(f)\n",
    "\n",
    "wrong_preds = [(val_set.dataset[i_dataset], incorrect_preds[\"values\"][i_self]) \n",
    "                for i_self, i_dataset in enumerate(incorrect_preds['indices'])]\n",
    "\n",
    "def decode_pair(pair):\n",
    "    junior = tokenizer.decode(token_ids=pair[0][\"input_ids\"][0, :], skip_special_tokens=True)\n",
    "    senior = tokenizer.decode(token_ids=pair[0][\"input_ids\"][1, :], skip_special_tokens=True)\n",
    "    return junior, senior\n",
    "decoded_wrong = [(decode_pair(w), w[1][1] - w[1][0], w[1]) for w in wrong_preds]\n",
    "\n",
    "decoded_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(decoded_wrong, columns=[\"Pair\", \"Difference\", \"Seniorities\"])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds[0][0][\"input_ids\"][0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(incorrect_preds[\"values\"]), len(incorrect_preds['indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100.).reshape(100, 1)\n",
    "np.where( x > 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Engineer II\"\n",
    "tokenized = tokenizer(text, return_tensors='pt').to('cuda')\n",
    "model.eval()\n",
    "model(**tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
