{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc55046-14e4-4c0b-bc07-e2757d722128",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98220cf3-032f-4a75-b965-76a6e656b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer\n",
    "from hr_research.config import output_path\n",
    "from datasets import load_dataset, Dataset\n",
    "from os.path import join\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f4b8f-6ecd-4b05-9cb4-fe5f810030a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-large\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30413c3b-664e-44cd-91b4-fbc338c97c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1751d-262b-44b9-b629-d3fdf680636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8742ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_data_train = join(output_path, \"mlm_data_train.jsonl\")\n",
    "mlm_data_val = join(output_path, \"mlm_data_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43349fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LEN = 22236629\n",
    "VAL_LEN = 2000\n",
    "# found experimentally. \n",
    "# all datapoints are contatenated during training, \n",
    "# therefore each sequence of len 512 is actually more than one datapoint\n",
    "AVG_DATAPOINT_LEN = 64\n",
    "ACTUAL_TRAIN_LEN = TRAIN_LEN // (512 // AVG_DATAPOINT_LEN)\n",
    "ACTUAL_VAL_LEN = VAL_LEN // (512 // AVG_DATAPOINT_LEN)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "NUM_STEPS_PER_EPOCH = ACTUAL_TRAIN_LEN // BATCH_SIZE\n",
    "EVALUATION_STEPS = (ACTUAL_VAL_LEN * 19) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7510ce3-dd02-4102-875f-8b270c556d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_dataset = load_dataset(\"json\", data_files={\n",
    "    'train': mlm_data_train, \n",
    "    'validation': mlm_data_val}, \n",
    "    streaming=True)\n",
    "\n",
    "mlm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fecbb5-9d48-4dcc-a57a-cee7e103e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = mlm_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71bc13-e36a-4812-801d-158d7f0d2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(next(iter(tokenized_datasets[\"train\"]))[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6fde8-ccb5-4c1f-ba9e-2ecf75b74aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35aa408-5535-423a-acec-3c83096458a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = list(t[\"input_ids\"] for t in itertools.islice(tokenized_datasets[\"train\"], 3))\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples):\n",
    "    print(f\"'>>> Resume {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8959d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = list(t[\"input_ids\"] for t in itertools.islice(tokenized_datasets[\"train\"], 3))\n",
    "\n",
    "lens = 0\n",
    "for idx, sample in enumerate(tokenized_samples):\n",
    "    lens += len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d16f0-1089-47e6-a421-9644a23d89b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d589a1-77d2-4de2-844a-a471c4703977",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea1a01-a2f9-48ce-b4e8-7703972f8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(next(iter(lm_datasets[\"train\"]))[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabc01d-b7a9-48ce-b4be-6225cd6ab310",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c97d9e-72d8-4048-9b95-2978b8b36f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35eb1b-c089-459e-890a-84e41e2a425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(itertools.islice(lm_datasets[\"train\"], 1))\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307afda-e1ad-422c-b3e5-cfd25bcfa9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        print(mapping)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(itertools.islice(lm_datasets[\"train\"], 1))\n",
    "samples[0]['word_ids']\n",
    "batch = whole_word_masking_data_collator(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38625b32-bee9-4882-b203-88eb008a9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(itertools.islice(lm_datasets[\"train\"], 1))\n",
    "print(f\"\\n'<<< {tokenizer.decode(samples[0]['input_ids'])}\")\n",
    "\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "for i, chunk in enumerate(batch[\"input_ids\"]):\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8326087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(itertools.islice(lm_datasets[\"train\"], 1))\n",
    "tokenizer.decode(samples[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf93d3e-06c1-4e2f-8165-a2002e54ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47ad4b-5c65-4f74-b791-91a4563176fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, pipeline\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=EVALUATION_STEPS,\n",
    "    max_steps=NUM_STEPS_PER_EPOCH,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    logging_steps=EVALUATION_STEPS,\n",
    "    report_to=\"tensorboard\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
