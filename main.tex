\documentclass[14pt]{mmcs_article}
\usepackage[russian]{babel}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    urlcolor=black,
    citecolor=black
}
\usepackage{tablefootnote}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\pgfplotsset{
  every axis/.append style={
    font=\sffamily
  },
  every tick label/.append style={
    font=\sffamily
  },
  every axis label/.append style={
    font=\sffamily
  },
  every axis title/.append style={
    font=\sffamily
  }
}


%\graphicspath{{images/}}%путь к рисункам

\begin{document}

\include{TitulMag}% для работы магистра

\renewcommand{\contentsname}{Оглавление}

\tableofcontents

%=======================
\newpage
\section*{Введение}
\addcontentsline{toc}{section}{Введение}

Роль информационных систем в процессах рекрутинга и трудоустройства чрезвычайно велика.
В данной работе рассматриваются различные подходы к оптимизации данных процессов с помощью машинного обучения и нейронных сетей.

Одной из наиболее важных задач в процессе рекрутинга является сопоставление кандидатов с вакансиями. Иными словами, среди массива резюме и вакансий необходимо находить такие пары $\langle\text{вакансия},\text{кандидат}\rangle$, которые наиболее соответствуют друг другу (в идеальном случае --- такой кандидат будет принят на предлагаемую должность).
Данная работа рассматривает данную задачу с точки зрения рекрутера и сфокусирована на поиске кандидатов для определённой вакансии.

У рекрутинговых компаний зачастую присутствует некоторая база данных кандидатов в рамках CRM (candidate relationship management), либо ATS (applicant tracking system) систем.
Стоит отметить, что каждая из компаний использует свои собственные процессы и хранит данные в нестандартизированных форматах.

Таким образом, создание системы для поиска релевантных кандидатов по сути является задачей извлечения и анализа данных из одного или нескольких неструктурированных источников.

Система в таком случае должна возвращать ранжированный список кандидатов, предложенных рекрутеру для конкретной вакансии. Конечно, достичь полной автоматизации процесса поиска соискателей, которые наиболее вероятно будут приняты на определённую работу, достаточно трудно, поэтому рекрутер должен использовать систему как один из инструментов в своей деятельности, а не замену своим профессиональным суждениям.

К тому же, работа рекрутера не ограничивается только лишь приглашением самого подходящего кандидата на работу, так как зачастую приглашенные соискатели могут не пройти одно из собеседований, либо отказаться от предложения. Чтобы максимально эффективно использовать своё время, рекрутер делает так называемый <<screening>>, т.е. выходит на контакт с несколькими кандидатами, и приглашает на собеседование только тех, кто соответствует определённым критериям. При этом количество приглашённых людей рассчитывается <<с запасом>>, на тот случай, что собеседование будет пройдено не всеми из них.

Данные аспекты работы рекрутера вынесены из рассмотрения в данной работе. Рассматривается только задача поиска релевантных соискателей: разрабатываемый подход работает в качестве <<retrieval>> системы, то есть извлекает из базы данных набор резюме для дальнейшего рассмотрения рекрутером и последующей обработки.

Упрощенный процесс поиска кандидатов для вакансии, описанный выше, представлен на рис.~\ref{fig:candidate_search_process}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/candidate_search_process.pdf}
  \caption{\centering Процесс поиска кандидатов для вакансии}
  \label{fig:candidate_search_process}
\end{figure}

Данная работа предлагает улучшить процесс поиска кандидатов путём внедрения искусственного интеллекта. Очевидно, это приведёт к следующим положительным эффектам:

\begin{itemize}
  \item уменьшение количества времени, затрачиваемого рекрутером на поиск кандидатов;
  \item уменьшение времени, затрачиваемого на общение с кандидатами, которые могут не подойти для вакансии по тем или иным причинам
        (освобождение времени для общения с более подходящими кандидатами);
  \item более <<точечный>> выбор кандидатов, с которыми компания будет выходить на контакт, т.е. меньшее количество <<спама>> кандидатам;
  \item как итог, более эффективная работа компании, увеличение объёма успешно <<закрытых>> вакансий.
\end{itemize}

%=======================
\newpage
\section*{Постановка задачи}\label{problem_statement}
\addcontentsline{toc}{section}{Постановка задачи}

Целью работы является разработка системы для поиска релевантных кандидатов по вакансиям с использованием методов машинного обучения.

Для достижения данной цели были поставлены следующие задачи:
\begin{enumerate}
  \item Анализ специфики данных в сфере рекрутинга, исследование особенностей представления резюме и вакансий, определение требований к системе поиска кандидатов.
  \item Обзор существующих подходов к семантическому поиску и ранжированию текстовых данных, анализ их применимости к задаче сопоставления резюме и вакансий.
  \item Разработка и реализация эмбеддинговой модели для семантического поиска кандидатов, учитывающей бизнес-процессы в сфере рекрутинга.
  \item Создание системы фильтрации и ранжирования резюме с учётом опыта кандидатов.
  \item Разработка и реализация API для работы с системой поиска кандидатов.
  \item Оценка качества работы системы, анализ результатов и выявление возможных направлений для улучшения.
\end{enumerate}


%=======================
\newpage
\section*{Обзор литературы}\label{literature_review}
\addcontentsline{toc}{section}{Обзор литературы}

\subsection*{Трансформеры}\label{transformers}

Трансформеры — это архитектура нейронных сетей, которая изменила подход к обработке естественного языка (natural language processing, NLP). Они были впервые представлены в статье Attention Is All You Need (Vaswani et al., 2017) \cite{vaswani2023attentionneed} и до сих пор производят непревзойдённые результаты в области NLP. Основным новшеством трансформеров является механизм самовнимания (self-attention), который позволяет модели одновременно анализировать все слова в предложении, определяя их взаимосвязи. Это делает трансформеры особенно эффективными для задач, требующих понимания контекста, таких как перевод, суммаризация и классификация текстов. В отличие от рекуррентных нейронных сетей (RNN), трансформеры обрабатывают данные параллельно, что значительно ускоряет обучение и обработку больших объемов текста.

Трансформеры стали основой для множества современных моделей NLP, включая BERT, RoBERTa, GPT и их производных.

\subsection*{BERT}\label{bert}

BERT (Bidirectional Encoder Representations from Transformers) --- это модель, представленная в статье BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al.,2018) \cite{devlin2019bertpretrainingdeepbidirectional} от компании Google. BERT обучается на больших текстовых корпусах, таких как книги и статьи из Википедии, с использованием двух задач: masked language modeling (предсказание случайно скрытых слов в предложении) и next sentence prediction (определение, являются ли два предложения последовательными). Благодаря двунаправленному вниманию, которое учитывает как левый, так и правый контекст, BERT создает глубокие представления слов и предложений, которые можно адаптировать для различных задач NLP.

В классификации трансформеров модели семейства BERT являются <<энкодерами>>, то есть они преобразуют входные данные (текст) в набор векторных представлений. Это отличает их от <<декодеров>>, которыми являются, например, GPT \cite{radford2018improving}, которые более оптимизированы для генерации текста\footnote{Стоит отметить, что любую архитектуру трансформера (энкодер/декодер) в принципе можно адаптировать для решения различных задач, однако достижимый ими результат будет отличаться. Например, ничего не мешает использовать BERT для генерации текста, но результат будет неудовлетворительным.}.

BERT широко используется в задачах, связанных с анализом текстов (таких как извлечение информации из вакансий и резюме), зачастую являясь базовой моделью, дообучаемой на данных из какой-либо предметной области.

\subsection*{JobBERT}\label{jobbert}

JobBERT --- это модель, основанная на базовой версии BERT, которая была дополнительно дообучена на корпусе из $\sim$3,2 миллиона предложений из текстов вакансий. Эта модель представлена в статье SkillSpan: Hard and Soft Skill Extraction from Job Postings (Zhang et al., 2022) \cite{zhang-etal-2022-skillspan}. Работа фокусируется на извлечении жестких навыков (<<hard skills>> --- технических умений, таких как программирование на языке Python) и мягких навыков (<<soft skills>> --- персональных качеств, таких как коммуникабельность) из объявлений о вакансиях.

Дообучение на данных из сферы рекрутинга позволяет JobBERT лучше понимать терминологию и структуру текстов, связанных с рынком труда. \cite{yacenko2023}

\subsection*{JobBERTa}\label{jobberta}

JobBERTa --- это модель, основанная на RoBERTa, которая также была дообучена на $\sim$3,2 миллиона предложений из вакансий. Она представлена в статье NNOSE: Nearest Neighbor Occupational Skill Extraction (Zhang et al., 2024) \cite{zhang-etal-2024-nnose}. В этой работе авторы предлагают метод извлечения профессиональных навыков с использованием подхода ближайших соседей в пространстве эмбеддингов. Этот метод позволяет идентифицировать навыки, которые семантически близки, даже если они не указаны в стандартных таксономиях профессий.

Так как JobBERTa основана на RoBERTa, которая в свою очередь является <<улучшенной>> версией BERT, то JobBERTa ожидаемо является более совершенной базовой моделью для дообучения на задачах, связанных с рынком труда, нежели JobBERT.

\subsection*{Sentence Transformers}\label{sentence_transformers}

Sentence Transformers — это библиотека, которая упрощает генерацию эмбеддингов для целых предложений или текстов. Она представлена в статье Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks (Reimers \& Gurevych, 2019) \cite{reimers2019sentencebertsentenceembeddingsusing}. В том время как стандартный BERT генерирует эмбеддинги на уровне слов, Sentence Transformers используют дополнительные слои, надстроенные поверх базовой архитектуры BERT, чтобы получить единый эмбеддинг всего текста. Обучается такая модель как сиамская нейронная сеть --- две или три идентичные модели типа BERT (возможно, использующие общий обучаемый набор весов) тренируются совместно, и произведенные эмбеддинги каждой из моделей сравниваются на схожесть. Этот процесс показан на рисунке \ref{fig:sentence_transformers_visualization}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
      box/.style={draw, rounded corners, minimum width=2cm, minimum height=1cm, align=center, font=\sffamily},
      arrow/.style={-latex}
    ]

    % Nodes
    \node[box] (cosine) {cosine sim(u, v)};
    \node[box, below=1cm of cosine, xshift=-2.5cm] (u) {u};
    \node[box, below=1cm of cosine, xshift=2.5cm] (v) {v};
    \node[box, below=1cm of u] (pooling1) {pooling};
    \node[box, below=1cm of v] (pooling2) {pooling};
    \node[box, below=1cm of pooling1] (bert1) {BERT};
    \node[box, below=1cm of pooling2] (bert2) {BERT};
    \node[below=0.5cm of bert1] (sentenceA) {Предложение A};
    \node[below=0.5cm of bert2] (sentenceB) {Предложение B};

    % Arrows
    \draw[arrow] (u) -- (cosine);
    \draw[arrow] (v) -- (cosine);
    \draw[arrow] (pooling1) -- (u);
    \draw[arrow] (pooling2) -- (v);
    \draw[arrow] (bert1) -- (pooling1);
    \draw[arrow] (bert2) -- (pooling2);
    \draw[arrow] (sentenceA) -- (bert1);
    \draw[arrow] (sentenceB) -- (bert2);

    % Label
    \node[above=0.3cm of cosine] {-1 \ldots 1};

  \end{tikzpicture}

  \vspace{0.8cm}
  \caption{\centering Визуализация работы Sentence Transformers}
  \label{fig:sentence_transformers_visualization}
\end{figure}


Эти эмбеддинги идеально подходят для задач, требующих сравнения текстов по смыслу, таких как семантический поиск, кластеризация и, собственно, сопоставление резюме с вакансиями. Библиотека предоставляет предобученные модели и инструменты для их дообучения.

\subsubsection*{Sentence Transformers и рынок труда}\label{sentence_transformers_application}

Практическое применение эмбеддинговых моделей в индустрии, например, демонстрирует компания Joveo в статье Using SentenceBERT to Generate Job Embeddings for Applications at Joveo \cite{joveo_sentencebert_2022} в рамках своего блога.

Компания Joveo применяет модель Sentence-BERT (SBERT) для генерации векторных представлений (эмбеддингов) вакансий, что позволяет эффективно решать задачи классификации профессий, определения схожести вакансий и прогнозирования пользовательских взаимодействий, таких как клики и отклики на вакансии. Этот подход направлен на улучшение обработки и категоризации неразмеченных данных о вакансиях, поступающих из различных источников.

В качестве базовой модели использовалась RoBERTa, которая была дообучена на проприетарном наборе данных, включающем более 500 000 вакансий. Каждая вакансия была классифицирована в соответствии с таксономией ONET SOC (\url{https://www.onetcenter.org/taxonomy.html}).

Эффективность эмбеддингов, созданных с помощью SBERT, оценивалась
с использованием метода обнаружения аномалий, основанного на показателе компактности кластеров. Этот показатель измеряет среднее попарное семантическое сходство внутри кластеров:

\begin{equation}
  \label{eq:compactness_cluster}
  \centering
  \allowdisplaybreaks[1]
  \begin{split}
    c(w) = \frac{1}{n(n-1)} \sum_{w_i \in W \setminus \{w\}} \sum_{w_j \in W \setminus \{w\}, w_j \neq w_i} sim(w_i, w_j)
  \end{split}
\end{equation}

где:
\begin{itemize}
  \item \( c(w) \) — показатель компактности для работы \( w \).
  \item \( n \) — количество работ в кластере, за исключением одной аномалии.
  \item \( W \) — множество работ, \( W = \{w_1, w_2, \ldots, w_{n+1}\} \).
  \item \( w_i, w_j \) — отдельные работы в множестве \( W \).
  \item \( sim(w_i, w_j) \) — семантическое сходство между работами \( w_i \) и \( w_j \).
\end{itemize}

Результаты показали улучшение производительности на 15\% по сравнению с базовыми эмбеддингами word2vec, что подчеркивает способность SBERT лучше улавливать семантические особенности текстов вакансий.


%=======================
\newpage
\section{Специфика данных}\label{data_specification}

Основные объекты, с которыми ведётся работа в рамках данного исследования --- резюме и вакансии. По своей природе это текстовые данные, не имеющие строгой структуры (очевидно, не существует единого шаблона для резюме).

Тем не менее, некоторые общепринятые понятия, используемые в сфере трудоустройства, можно выделить, и на основе них построить структуру данных. На основе таких общих понятий существует некоторое множество попыток стандартизировать представление резюме и вакансий, зачастую являющиеся XML-схемами (например: \url{https://xmlresume.sourceforge.net/}). Стоит отметить, что ни одна из таких схем не является универсальным стандартом.

В целом, большинство структурированных представлений резюме содержат следующие поля:

\begin{itemize}
  \item Заголовок резюме;
  \item Описание деятельности;
  \item Образование;
  \item Умения (skills);
  \item Опыт работы; несколько записей следующего вида:
        \begin{itemize}
          \item Название должности;
          \item Описание должности;
          \item Дата начала работы;
          \item Дата окончания работы;
          \item Информация о компании;
        \end{itemize}
  \item Сертификаты;
  \item Знание языков;
  \item Персональная и контактная информация;
\end{itemize}

Вакансии намного более сложны в плане структуризации, нежели резюме, так как они зачастую более разнообразны. Однако можно выделить следующие часто встречающиеся поля:

\begin{itemize}
  \item Название позиции;
  \item Описание позиции;
  \item Требования (requirements);
  \item Условия (зарплата, график работы, бенефиты);
  \item Контактная информация;
\end{itemize}

Для извлечения указанной информации часто используются так называемые <<парсеры>> --- системы, способные принимать документы во множестве форматов (PDF, DOCX, HTML и т.\,д.) и возвращать их структурированное представление.

Парсинг резюме и вакансий --- отдельная сложная задача, которая не рассматривается в данной работе. Для получения структурированных резюме и вакансий использовался API стороннего сервиса.

\subsection*{Фильтрация и ранжирование}\label{filtering_and_ranking}

Исходя из специфики данных, можно разделить задачу поиска релевантных кандидатов на две подзадачи:

\begin{itemize}
  \item Фильтрация резюме;
  \item Ранжирование резюме;
\end{itemize}

Фильтрация заключается в том, чтобы отсеять резюме по каким-либо жестким критериям, например, если требуется некоторое количество лет опыта с какой-либо технологией или проживание в определённом регионе.

Затем среди отфильтрованных резюме производится ранжирующий поиск. Этот процесс отражён на рис.~\ref{fig:filtering_and_ranking}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/filtering_and_ranking.pdf}
  \caption{\centering Процесс фильтрации и ранжирования резюме}
  \label{fig:filtering_and_ranking}
\end{figure}

Инструмент фильтрации имеет свои недостатки.

Одним из таких недостатков является то, что не все кандидаты указывают в своём резюме всю необходимую информацию. Например, если для работы требуются водительские права, то возможность отсеять кандидатов по наличию прав является крайне полезной, но в то же время фильтрация может отсеять подходящего кандидата, поскольку не всегда их наличие указано.

Иными словами, если вы пишете резюме на работу, связанную с вождением, то вы рассчитываете на то, что обладание водительскими правами понятно из контекста, и не указываете факт их наличия, таким образом не проходя фильтрацию.

Исходя из этого, можно сделать вывод, что фильтрация является сложной задачей, которая требует от системы хорошего понимания содержимого резюме. Иначе, существует риск отсеять кандидата, который на самом деле является подходящим для вакансии.

Это можно нивелировать, если делать так называемые <<мягкие фильтры>>, которые будут участвовать в ранжировании, но не будут жёстко отсеивать кандидатов. Мягкие фильтры также в какой-то мере отражают то, как рекрутер рассматривает резюме: например, если для работы необходимо пять лет опыта работы в какой-то сфере, то резюме человека, работающего в этой сфере четыре года, будет рассмотрено, и если кандидат обладает достаточной экспертизой, то он будет продвинут в списке кандидатов.

Однако в постановке задачи, рассматриваемой в данной работе, не рассматривается подход <<мягких фильтров>> по следующим причинам:

\begin{itemize}
  \item это усложняет задачу ранжирования, так как необходимо учитывать предпочтения рекрутера (допустим, если требование к годам опыта работы не является жёстким, то актуальным становится вопрос, какую функцию использовать для снижения веса резюме кандидата с опытом, отличным от требуемого на N лет);
  \item <<мягкие фильтры>> являются неочевидными с точки зрения UX, так как пользователи зачастую приходят в замешательство, видя кандидата, который явно не соответствует введённым критериям;
\end{itemize}

Фильтрация нечётким образом могла бы быть <<спрятана>> в ранжирующей модели (например, с помощью преобразования вакансии в набор фильтров с помощью LLM), что позволило бы избежать проблем с UX, однако это является плохим бизнес-решением, так как рекрутеры желают иметь максимально точный контроль над системой.

Таким образом, был выбран подход с жёсткими фильтрами, так как он легко объясним с точки зрения UX (каждый человек ожидает от фильтров именно жёсткого поведения, ведь они повсеместно встречаются в Интернете: например, в интернет-магазинах) и лёгок в реализации.

Ранжирование не настолько тривиально, как жёсткая фильтрация, и может быть реализовано различными способами. Данная работа рассматривает только один из них --- использование нейронных сетей для генерации векторных представлений резюме и вакансий.


%=======================
\newpage
\section{Эмбеддинговые модели и векторный поиск}\label{embedding_models}

Так как и вакансия, и резюме являются структурированными текстовыми данными, то для их обработки можно использовать методы NLP (natural language processing).

При этом налагаются следующие ограничения:

\begin{itemize}
  \item каждый запрос должен быть достаточно быстрым и дешёвым; система не может позволить себе оценивать каждого кандидата с помощью LLM;
  \item обработка каждого резюме не должна быть слишком дорогой, т.к. зачастую компании хранят у себя миллионы резюме;
  \item ранжирование должно быть <<умным>>, т.е. учитывать семантику текстов. Алгоритмы, основанные на текстовом совпадении (например, BM-25) являются недостаточно хорошим решением для этой задачи.
\end{itemize}

Исходя из этих ограничений, был выбран подход с использованием эмбеддинговых моделей и векторного поиска.

Эмбеддинговые модели позволяют представить текст в виде вектора чисел фиксированной размерности (обычно от 384 до 1536 измерений). Основная идея такого представления заключается в том, что семантически близкие тексты должны иметь близкие векторные представления.

Например, резюме двух Python-разработчиков с похожим опытом работы должны иметь близкие векторные представления, в то время как резюме Python-разработчика и бухгалтера должны иметь далекие векторные представления.

На рис.~\ref{fig:embedding_visualization} представлена упрощённая визуализация векторных представлений резюме, спроецированных на двумерное пространство. Как видно из рисунка, резюме со схожими навыками и опытом работы располагаются близко друг к другу.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        xlabel={Первая компонента},
        ylabel={Вторая компонента},
        scatter/classes={
            python={mark=*,blue},
            java={mark=*,red},
            accounting={mark=*,green}
          }
      ]
      % Python developers cluster
      \addplot[scatter,only marks,scatter src=explicit symbolic] coordinates {
          (-0.6,0.3) [python]
          (-0.4,0.2) [python]
          (-0.2,0.6) [python]
        };
      % Java developers cluster
      \addplot[scatter,only marks,scatter src=explicit symbolic] coordinates {
          (-1,1) [java]
          (-1.2,0.8) [java]
          (-0.8,1.2) [java]
        };
      % Accountants cluster
      \addplot[scatter,only marks,scatter src=explicit symbolic] coordinates {
          (0,-1) [accounting]
          (0.2,-1.2) [accounting]
          (-0.2,-1.2) [accounting]
        };
      \legend{Python-разработчики,Java-разработчики,Бухгалтеры}
    \end{axis}
  \end{tikzpicture}
  \caption{\centering Визуализация векторных представлений резюме в двумерном пространстве}
  \label{fig:embedding_visualization}
\end{figure}

Это свойство позволяет свести задачу поиска похожих текстов к задаче поиска близких векторов в многомерном пространстве. При этом близость векторов может быть измерена с помощью метрик, таких как косинусное расстояние или евклидово расстояние.

Важным преимуществом такого подхода является то, что векторное представление текста может быть вычислено один раз для одного резюме и затем использовано многократно. Это позволяет значительно сократить время и стоимость поиска, так как нет необходимости каждый раз заново анализировать текст.

Кроме того, векторные представления позволяют учитывать семантические связи между словами и фразами. Например, если в вакансии требуется опыт работы с <<Python>>, то резюме кандидата с опытом работы с <<Django>> также может получить высокий ранг, так как векторные представления этих технологий близки (Django является Python-фреймворком).

Для решения поставленной задачи была выбрана модель JobBERTa в совокупности с библиотекой Sentence Transformers.

\subsection*{Обучение эмбеддинговых моделей}

Эмбеддинговые модели часто требуют дообучения (fine-tuning) на конкретной задаче, так как базовые предобученные веса не всегда достигают оптимального результата. Для того чтобы модель могла генерировать качественные векторные представления для требуемой области, необходимо обучить её на наборе данных из этой области.

Одним из наиболее эффективных подходов к обучению эмбеддинговых моделей является метод Triplet Loss. Этот метод основан на идее обучения на триплетах (тройках), состоящих из следующих элементов:
\begin{itemize}
  \item Anchor (якорь) --- базовый элемент, относительно которого происходит сравнение
  \item Positive (позитивный) --- элемент, семантически близкий к якорю
  \item Negative (негативный) --- элемент, семантически далёкий от якоря
\end{itemize}

Цель обучения заключается в том, чтобы расстояние между векторными представлениями якоря и позитивного примера было меньше, чем расстояние между якорем и негативным примером на некоторую величину (margin). Математически это можно выразить следующим образом:

\begin{equation*}
  \centering
  \allowdisplaybreaks[1]
  \begin{split}
    d(a,p) + margin < d(a,n)
  \end{split}
\end{equation*}

где $d(x,y)$ --- функция расстояния между векторами, $a$ --- якорь, $p$ --- позитивный пример, $n$ --- негативный пример.

Закономерно возникает вопрос составления датасета троек (anchor, positive, negative) для обучения.

\subsubsection{Генерация позитивных примеров}\label{positive_examples_generation}

Так как разметка подобных данных дорога (не каждый человек сможет разметить степень того, насколько кандидат подходит вакансии, необходим опыт в HR-сфере), было принято решение извлекать разметку из самих данных (self-supervised подход), без валидации человеком.

Для генерации позитивных примеров в данной работе использовался подход, основанный на анализе карьерного пути кандидатов. Предполагается, что успешные переходы между позициями в карьере человека указывают на схожесть требуемых навыков и компетенций между этими позициями.

При таком подходе каждая новая позиция в карьере кандидата рассматривается как <<вакансия>> (anchor), а предыдущий опыт работы --- как позитивный пример (positive) (рис.~\ref{fig:career_path_example}). Это позволяет автоматически формировать пары семантически близких резюме на основе реальных карьерных переходов.

Очевидно, что люди могут спонтанно сменить род деятельности (например, стать садовником после долгих лет работы программирования на C++), однако во время экспериментов было установлено, что подобный шум не вносит значительных искажений в результат работы модели. К тому же, когда люди составляют резюме для какой-либо вакансии, они включают только относящийся к ней опыт работы и опускают остальной.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/career_path_example.pdf}
  \caption{\centering Примеры формирования нескольких пар anchor-positive на основе одного карьерного пути}
  \label{fig:career_path_example}
\end{figure}

Например, как показано на рис.~\ref{fig:career_path_example}, если кандидат успешно продвигался по карьерной лестнице от Junior до IT Business Analyst, то можно считать, что его опыт работы на позиции Junior Python Developer является релевантным для позиции Python Backend Engineer. Также релевантным для позиции IT Business Analyst являлся весь предыдущий опыт, то есть пара $\langle\text{Junior Python Developer},\text{Python Backend Engineer}\rangle$.

Такой подход имеет несколько преимуществ:
\begin{itemize}
  \item Автоматическая генерация большого количества обучающих примеров из реальных данных (а именно N-1 датапоинтов для опыта человека, состоящего из N позиций);
  \item Поиск реальных семантических связей (модель не просто выучит, что Python Developer это хороший кандидат для вакансии Python Developer, но и найдёт связь между Software Engineer и Python Developer).
\end{itemize}

Таким образом был составлен обучающий датасет, состоящий из пар вида $\langle\text{anchor},\text{positive}\rangle$. Также часть резюме была использована для формирования тестовой выборки (ни один из датапоинтов, полученных из таких резюме, не был задействован в обучающей выборке).

\subsubsection{Аугментация позитивных примеров}

Во время валидации модели были обнаружены некоторые проблемы, требующие решения и переосмысления подхода.
Так как положительный пример и поисковый запрос всегда выбирались из одного резюме, модель переобучалась на стиль написания описаний и названий вакансий. Например, если рекрутер задаёт короткое описание, то будут найдены кандидаты, также не использующие большое количество слов в описаниях опыта.

Таким же образом модель переобучалась и на другие аспекты стиля: lowercase/uppercase, упоминания компаний (если человек множество раз менял должность в рамках одной компании, то это производило достаточно много пар с одинаковыми названиями компаний). Это крайне нежелательно, ведь в таком случае при поиске на работу в какую-либо компанию часть выдачи будет заполнена людьми, уже в ней работающими. Наиболее чётко данный эффект проявлялся тогда, когда кандидат использовал те же специальные символы, что и вакансия (например, звёздочки для обозначения буллит-списков).

Очевидно, что для построения хорошей обучающей выборки необходим препроцессинг, делающий полученную модель инвариантной к стилю написания и анонимизирующий сущности, на которые она может переобучиться.

Этап анонимизации и «стирания» стиля может быть реализован непосредственно перед эмбеддинговой моделью с использованием LLM, что, безусловно, добавит к полученному пайплайну стоимости, так как будет необходим инференс LLM для каждого кандидата.

Было решено не прибегать в препроцессингу с помощью LLM и использовать следующие методы:

\begin{itemize}
  \item Полное удаление специальных символов, <<схлопывание>> множественных пробелов и переносов строк;
  \item Приведение к lowercase;
  \item Использование аугментаций из библиотеки nlpaug\cite{ma2019nlpaug}.
\end{itemize}

Также рассматривалась возможность использовать NER-подход для нахождения и маскирования PII (personally identifiable information) и названий компаний. NER-подход не был реализован в виду своей сложности и трудоёмкости..

\subsubsection{Генерация негативных примеров}

Одной из основных проблем при использовании Triplet Loss является сложность формирования качественных обучающих триплетов. Если позитивные примеры можно относительно легко получить из реальных данных (например, с помощью указанного выше подхода, либо когда компания приняла кандидата на работу или рекрутер отметил резюме как релевантное), то с негативными примерами ситуация гораздо сложнее. Тот факт, что рекрутер пропустил или не рассмотрел какое-то резюме, не обязательно означает, что данный кандидат не подходит для позиции.

Для эффективного обучения необходимо использовать <<сложные>> негативные примеры (hard negatives)\cite{xuan2021hardnegativeexampleshard} --- те, которые достаточно похожи на якорь, но всё же должны быть классифицированы как отличающиеся. В контексте поиска резюме это могут быть, например, резюме специалистов из смежных, но всё же различных областей (например, для резюме Python-разработчика негативным примером может быть резюме системного администратора).

Исходя из подходов к формированию позитивных примеров, описанных в разделе \ref{positive_examples_generation}, поиск качественных негативных примеров становится ещё более сложной задачей. Так как позитивные примеры уже достаточно широкие (учитывая возможность людей менять род деятельности и переходить на смежные позиции), становится труднее найти примеры, которые можно однозначно считать более негативными по отношению к некоторым позитивным.

В рамках данной работы единственным реализуемым способом генерации негативных примеров оказался случайный выбор (random sampling) внутри батча --- резюме, являющиеся позитивными примерами для других якорей в батче, рассматриваются как негативные для текущего якоря. Этот подход продемонстрирован на рисунке \ref{fig:batch_triplets}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/batch_triplets.pdf}
  \caption{\centering Пример формирования батча триплетов из 3-х пар: положительные примеры соседних в батче пар становятся отрицательными для текущего якоря}
  \label{fig:batch_triplets}
\end{figure}

Такой подход к формированию негативных примеров в сочетании с широкими позитивными примерами приводит к появлению ложных отрицательных примеров (false negatives). То есть в некоторых случаях реальный рекрутер мог бы поменять местами позитивный и негативный примеры, так как негативный пример на самом деле более релевантен для данной вакансии. Это является одним из существенных ограничений используемого подхода.

Были исследованы иные подходы к генерации негативных примеров, например, использование более <<сильной>> эмбеддинговой модели (такой как embedding модели от OpenAI) для фильтрации негативных примеров --- если модель оценивает, что anchor и negative уже достаточно близки, значит это скорее всего ложный негативный пример.

Такие подходы не принесли значительных результатов (значение тестовых метрик не изменилось в сравнении с использованием случайного выбора). Возможно, это связано с тем, что:

\begin{itemize}
  \item Вероятность появления ложных отрицательных примеров достаточно низкая, и их влияние не столь велико;
  \item Благодаря случайному составлению батчей, на каждой эпохе якорям сопоставляются разные негативные примеры, что не позволяет модели переобучиться на <<неверный>> пример;
  \item Попытки избавиться от ложных отрицательных примеров могут отфильтровывать сложные отрицательные примеры (hard negatives), являющиеся крайне полезными для обучения.
\end{itemize}

%=======================
\newpage
\section{Асимметричный поиск}\label{assymetric_search}

В предыдущем разделе был рассмотрен подход к семантическому поиску с использованием эмбеддинговых моделей. Однако, такой подход имеет существенное ограничение --- симметричность метрики косинусного расстояния, используемой для сравнения эмбеддингов. В контексте задачи сопоставления резюме и вакансий это ограничение становится критическим, так как отношения между позициями часто являются асимметричными.

Рассмотрим простой пример: переход от позиции <<Middle Software Engineer>> к <<Senior Software Engineer>> является логичным и желательным с точки зрения карьерного роста. Однако обратный переход --- от <<Senior>> к <<Middle>> --- является нежелательным и даже может быть воспринят как понижение в должности. При использовании традиционного подхода с косинусным расстоянием оба этих перехода будут иметь одинаковую оценку близости, что не соответствует реальной ситуации.

Таким образом, необходимо учитывать не только семантическую близость позиций, но и их относительный <<уровень>> или <<старшинство>>.

Контроль над этим параметром может быть хорошим инструментом для рекрутера, так как в этом случае он смог бы фокусировать поиск на более или менее опытных кандидатах, в зависимости от специфики вакансии.

\subsection{Требования к асимметричной метрике}

Описанная выше проблема была решена с помощью введения бинарного отношения на множестве позиций ($>_{s}$), которое связывает два элемента множества позиций, если первый элемент является более опытным или старшим по отношению ко второму. Это отношение должно удовлетворять следующим требованиям:

\begin{itemize}
  \item Транзитивность: \\
        \begin{equation*}
          \centering
          \allowdisplaybreaks[1]
          \begin{split}
            \text{Senior developer} & >_{s} \text{Middle developer} \\
                                    & ~~~\land                      \\
            \text{Middle developer} & >_{s} \text{Junior developer} \\
                                    & \implies                      \\
            \text{Senior developer} & >_{s} \text{Junior developer}
          \end{split}
        \end{equation*}
  \item Не все элементы множества позиций должны быть сравнимы друг с другом. Например, трудно определить, является ли человек, занимающий позицию <<Middle developer>>, более опытным, чем <<Business Analyst>>.
  \item Возможность быстрой проверки того, связывает ли отношение два элемента. Кросс-энкодинг, то есть запуск нейронной сети для каждой пары $\langle \text{вакансия}, \text{резюме} \rangle$ для выявления связи между ними, является достаточно дорогостоящим, поэтому необходимо использовать метод, выполняющий основную массу вычислений на этапе загрузки данных в базу, а не при каждом запросе. Такое свойство имеет метод векторного поиска с помощью описанной выше эмбеддинговой модели (эмбеддинг строится при добавлении данных, а простая операция расчёта косинусного расстояния выполняется во время запроса).
\end{itemize}

Было замечено, что такое отношение может быть реализовано в качестве фильтра в рамках процесса, описанного в разделе \ref{filtering_and_ranking}, то есть кандидаты могут быть отфильтрованы с помощью отношения $>_{s}$, а ранжирование может осуществляться эмбеддинговой моделью.

Таким образом, для решения проблемы асимметричности разработан подход, состоящий из двух компонентов:

\begin{itemize}
  \item Базовая модель семантического поиска, которая находит кандидатов с опытом, схожим с требуемым (модель из раздела \ref{embedding_models});
  \item Модель оценки <<старшинства>> (seniority), определяющая относительный уровень позиций в данном участке эмбеддингового пространства.
\end{itemize}

При этом отношение $>_{s}$ решено было выразить отношением <<меньше>> на вещественных числах от 0 до 1 (сравнение чисел транзитивно, выполняется быстро, а эмбеддинговое ранжирование позволяет сравнивать между собой только близкие позиции).

Иными словами, наряду с компонентами эмбеддингов добавляется новая компонента, которая позволяет отфильтровать кандидатов, не соответствующих требованиям по опыту.

Данный подход отражён на рис.~\ref{fig:assymetric_search}.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        view={45}{35},
        axis lines=center,
        xlabel={Первая компонента},
        ylabel={Вторая компонента},
        zlabel={Seniority},
        xlabel style={below right},
        ylabel style={below right},
        zlabel style={rotate=90},
        zmin=0,
        xmin=0.5,
        xmax=1.5,
        ymin=0.5,
        ymax=2.0,
        zmax=2.5,
        xtick=\empty,
        ytick=\empty,
        ztick=\empty,
        width=0.8\textwidth,
        height=0.8\textwidth,
      ]
      % Base points (on the "floor")
      \addplot3[only marks,mark=*,mark size=3pt,color=black] coordinates {
          (1,1,0)  % Python Intern
          (0.7,1.5,0)  % Senior Python Dev
          (0.6,0.7,0)  % Backend Dev
        };

      % Arrows representing seniority
      \addplot3[->,thick,color=red] coordinates {
          (1,1,0) (1,1,0.5)  % Python Intern arrow
        };
      \addplot3[->,thick,color=red] coordinates {
          (0.7,1.5,0) (0.7,1.5,2.0)  % Senior Python Dev arrow
        };
      \addplot3[->,thick,color=red] coordinates {
          (0.6,0.7,0) (0.6,0.7,1.0)  % Backend Dev arrow
        };

      % Labels
      \node[anchor=north west] at (axis cs:1,1,0) {Python Intern};
      \node[anchor=north west] at (axis cs:0.7,1.5,0) {Senior Python Dev};
      \node[anchor=north west] at (axis cs:0.6,0.7,0) {Backend Dev};
    \end{axis}
  \end{tikzpicture}
  \caption{\centering Визуализация асимметричного поиска: точки в эмбеддинговом пространстве с учётом уровня опыта (seniority)}
  \label{fig:assymetric_search}
\end{figure}

\subsection{Данные для модели асимметричной оценки}

Для обучения модели был собран датасет, также использующий self-supervised подход. Датасет является набором пар позиций, связанных отношением $>_{s}$.

Эти позиции получены следующим образом:

\begin{itemize}
  \item Была подготовлена база дедуплицированных резюме, где каждая позиция в опыте кандидата была нормализована (приведена к нижнему регистру, удалены спецсимволы и лишние пробелы).
  \item Была создана матрица размера $n \times n$, где $n$ --- количество уникальных позиций в базе резюме. Каждый раз, когда позиция $p_1$ встречалась после позиции $p_2$ в опыте кандидата, элемент матрицы $M[p_1, p_2]$ увеличивался на 1.
  \item После этого для каждой неупорядоченной пары $\langle p_1, p_2 \rangle$ был посчитан коэффициент $k_{p_1, p_2} = \frac{M[p_1, p_2]}{M[p_1, p_2] + M[p_2, p_1]}$. Этот коэффициент отражает то, насколько часто $p_1$ встречается после $p_2$ по отношению ко всем совместным вхождениям $p_1$ и $p_2$ в резюме, то есть более вероятна быть более старшей позицией.
  \item Были выбраны все пары позиций, для которых $k_{p_1, p_2}$ был выше определённого порога, а так же сумма $M[p_1, p_2] + M[p_2, p_1]$ была достаточно большой, чтобы исключить влияние шума.
  \item Была отфильтрована часть пар позиций, которые отличаются только указанием <<грейда>> (например, <<Python developer>> и <<Senior Python developer>>), так как они не несут достаточно полезной информации для обучения модели.
\end{itemize}

Примеры таких пар указаны в таблице \ref{tab:seniority_pairs}.

\begin{table}[H]
  \centering
  \caption{\centering Примеры пар позиций, связанных отношением $>_{s}$}
  \label{tab:seniority_pairs}
  \begin{tabular}{|l|l|}
    \hline
    \textbf{<<Младшая>> позиция} & \textbf{<<Старшая>> позиция}    \\
    \hline
    Property Manager             & Director of Property Management \\
    \hline
    Accounts Payable Clerk       & Staff Accountant                \\
    \hline
    Business analyst             & Business Analyst Product Owner  \\
    \hline
    Software Engineer            & Senior Database Administrator   \\
    \hline
    Technical Project Manager    & Senior IT Project Manager       \\
    \hline
  \end{tabular}
\end{table}

Из данного датасета была извлечена тестовая выборка. Она составлялась таким образом, чтобы в ней были позиции, которые не встречаются ни в каких отношениях в обучающей выборке.

Достигнуто это было следующим образом: было отобрано некоторое количество позиций, а затем все отношения, в которых они встречались, были помещены в тестовую выборку.

Тестирование на таких позициях крайне важно, так как проверяет способность модели обобщать данные, отслеживать переобучение на распределении конкретных известных позиций.

\subsection{Обучение модели асимметричной оценки}

Задача свелась к обучению модели регрессии, которая получает на вход одну позицию кандидата и возвращает численную оценку, отражающую уровень опыта.

Сложность обучения такой модели заключается в том, что она должна хранить данные о распределении всех возможных позиций внутри своих весов. Это нужно для того, чтобы одновременно анализируя только одну из двух позиций в паре, она могла произвести для каждой из этих позиций такую оценку, чтобы она была сравнима с оценкой другой позиции, и результат сравнения совпадал с отношением $>_{s}$.

В отличие от традиционных задач регрессии, датасет не содержит никаких конкретных чисел, а состоит только из пар позиций и отношений между ними.

Это своего рода уникальная постановка задачи, и может быть обобщена следующим образом:

\begin{itemize}
  \item $P$ --- множество с бинарным отношением $>_{P}$
  \item $I_{P}$ --- множество оценок с обычным отношением порядка $>$, при этом для удобства $I_{P} \subset [0, 1]$
  \item Полученная модель является функцией $f: P \rightarrow I_{P}$, сохраняющей порядок: если $p_{1} >_{P} p_{2}$, то $f(p_{1}) > f(p_{2})$
\end{itemize}

Таким образом, задача сводится к построению монотонного отображения (гомоморфизма упорядоченных множеств) из множества некоторых объектов в множество вещественных чисел $[0, 1]$ с обычным порядком.

При этом данные заданы только лишь парами объектов, удовлетворяющими $>_{P}$.

Оценка качества в таком случае тривиальна: точность (accuracy) на количестве верно предсказанных отношений тестовой выборки.

Однако обучение нетривиально, так как на момент выполнения работы не существовало подходящей лосс-функции для описанной задачи и формата данных.

В ходе экспериментов была разработана собственная лосс-функция, имеющая следующий вид:

\begin{equation}
  \label{eq:custom_loss_function}
  L(p, q) = \begin{cases}
    (p - 0)^2 + (q - 1)^2 & \text{если } \text{margin} + p - q \geq 0 \\
    0                     & \text{иначе}
  \end{cases}
\end{equation}

где:
\begin{itemize}
  \item $p$ --- оценка модели для младшей позиции
  \item $q$ --- оценка модели для старшей позиции
  \item $\text{margin}$ --- константа, определяющая минимально допустимую разницу между оценками
\end{itemize}

Данная лосс-функция пытается <<приблизить>> оценку младшей позиции к нулю, а старшей к единице, если отношение между ними не выполняется с учётом margin.

Реализация процесса обучения с данной лосс-функцией на языке Python с использованием библиотеки HuggingFace \cite{wolf2020huggingfacestransformersstateoftheartnatural} представлена в приложении (листинг \ref{app-lst:1}).

У данной лосс-функции, однако, есть один существенный недостаток: Очень медленная сходимость из-за того, что если модель верна, то обучение не происходит вовсе (лосс-функция равна 0). Когда модель становится достаточно хороша и предсказывает >90\% отношений, то огромная часть вычислительных ресурсов, затраченных на инференс модели, не приносит никакой пользы.

Бороться с этим недостатком трудно, так как если штрафовать модель за недостаточную <<уверенность>> в предсказании, то есть продолжать приближать оценки к 0 и 1 даже при правильном предсказании, это приведёт к тому, что модель не сможет выучить распределение оценок.

Тем не менее, с данной лосс-функцией удалось достичь точности 0.95 на тестовой выборке, то есть модель смогла предсказать более 95\% отношений между позициями.

При этом, обучение модели происходило в течение сотен эпох, что является крайне неэффективным.


\newpage
\section{Ранжирование с учётом опыта}\label{experience_ranking}

Как было показано в предыдущем разделе, наличие отдельного фильтра, позволяющего сравнивать <<старшинство>> позиций, является удобным инструментом для рекрутеров.

Однако, этот инструмент имеет следующие недостатки:

\begin{itemize}
  \item Продемонстрированный способ позволяет сравнивать только пары позиций. Однако резюме являет собой список позиций. Так как модель асимметричной оценки возвращает численную оценку, то результаты её работы можно агрегировать (например, вычислять среднюю оценку по всем позициям в резюме). Очевидно, что более <<интеллектуальный>> метод оценки всего резюме (а не агрегации отдельных позиций) способен производить более точные результаты.
  \item Для реализации предложенного фильтра требуется запустить модель такого же размера, что и эмбеддинговая модель, для каждой позиции из резюме, что довольно затратно.
  \item Обучение модели асимметричной оценки крайне неэффективно.
\end{itemize}

Обозначенная в разделе \ref{assymetric_search} проблема, побудившая разработку модели асимметричной оценки, может быть решена и с помощью другого подхода: если в формате данных для модели сообщить достаточно информации о том, является ли строка резюме или <<вакансией>>\footnote{<<Вакансия>> взята в кавычки, так как в разделе \ref{positive_examples_generation} было показано, что на этапе обучения <<вакансии>> являются псевдо-вакансиями, извлекаемыми из резюме.}, то модель сможет определить, что более <<старшему>> резюме не следует сопоставлять более <<младшие>> вакансии.

Иными словами, если ранее решалась проблема того, что данное равенство нежелательно, но неизбежно вследствие симметричности косинусного расстояния:

\begin{equation}
  \label{eq:assymetric_search_problem}
  \begin{aligned}
    d(\text{junior developer}, & \text{senior developer}) \\
                               & =                        \\
    d(\text{senior developer}, & \text{junior developer})
  \end{aligned}
\end{equation}

то теперь возможно уйти от данного равенства:

\begin{equation}
  \label{eq:solved_assymetric_search_problem}
  \begin{aligned}
    d(r_{f}(\text{junior developer}), & j_{f}(\text{senior developer})) \\
                                      & \neq                            \\
    d(r_{f}(\text{senior developer}), & j_{f}(\text{junior developer}))
  \end{aligned}
\end{equation}

где:
\begin{itemize}
  \item $r_{f}$ --- функция, преобразующая резюме в формат, понятный модели,
  \item $j_{f}$ --- функция, преобразующая вакансию в формат, понятный модели,
\end{itemize}

при этом оба формата должны быть достаточно различимы, чтобы модель могла <<классифицировать>>,  с чем имеет дело во время инференса. Приемлемо также иметь одни веса модели для кодирования вакансий, а другие для кодирования резюме, но это несёт дополнительные затраты в виде сложности разработки и не имеет необходимости.

Таким образом, получая на вход вакансию, модель будет <<осознавать>>, что имеет дело именно с вакансией, и будет сопоставлять ей точку в эмбеддинговом пространстве, вокруг которой будут находиться резюме с необходимым уровнем опыта.

\subsection*{Кодирование опыта кандидата}

Чтобы модель смогла правильно оценить опытность кандидата, необходимо включить в строку, подаваемой на вход модели, информацию о том, как долго кандидат работает на той или иной позиции.

Наивный способ это сделать состоит в том, чтобы включить годы работы, указанные в резюме. Данный способ демонстрирует таблица \ref{tab:position_encoding_example} на примере опыта кандидата, рассмотренного ранее на рисунке \ref{fig:career_path_example}.

\begin{table}[H]
  \centering
  \caption{\centering Пример включения в строку информации о длительности работы на позиции.}
  \label{tab:position_encoding_example}
  \begin{tabular}{|c|p{7cm}|p{7cm}|}
    \hline
    \textbf{№} & \textbf{Псевдо-вакансия (anchor)}                   & \textbf{Опыт кандидата (positive)}                                                                                                                                                                                                                                                                                                                                                                                                                                        \\
    \hline
    1          & IT Business Analyst <описание опыта на позиции>     & Junior Python Dev (2019-2020) [SEP]\tablefootnote{[SEP] --- специальный токен-разделитель, может быть опущен. RoBERTa, на которой основана JobBERTa, имеет другой формат токенов-разделителей, но здесь для простоты используется тот же, что и в BERT.} Python Backend Engineer (2020-2024) <описание опыта на позиции>\tablefootnote{Описание опыта на позиции включено только для последней (или текущей) позиции, так как размер входного массива токенов ограничен.} \\
    \hline
    2          & Python Backend Engineer <описание опыта на позиции> & Junior Python Dev (2019-2020) <описание опыта на позиции>                                                                                                                                                                                                                                                                                                                                                                                                                 \\
    \hline
  \end{tabular}
\end{table}

Данный способ имеет главный недостаток: такое представление лет опыта не является оптимальным для модели-трансформера, так как запись вида <<2020-2024>> будет представлена в виде множества токенов, что:

\begin{itemize}
  \item Заполняет и без того ограниченный входной набор токенов (например, в BERT можно поместить только 512 токенов).
  \item Неявно представляет самую важную метрику — длительность работы на позиции (модели придётся <<вычислять>> опыт).
\end{itemize}

Было решено провести feature engineering, чтобы представить опыт кандидата в виде, удобном для модели.

Для создания такого представления было сделано следующее:

\begin{itemize}
  \item Добавлен токен, отражающий, насколько давно была начата работа на данной позиции относительно текущего момента времени. Стоит отметить, что для генерации данных в соответствии с процессом, описанном в разделе \ref{positive_examples_generation}, <<текущий момент времени>> является моментом смены работы (моментом, когда кандидат начал работать на позиции, которая взята в качестве anchor).
  \item Добавлен токен, отражающий, сколько лет было проведено на данной позиции.
\end{itemize}

Нетрудно заметить, что для того, чтобы представить, например, количество проведенных лет на позиции одним токеном, необходимо каким-либо образом представить непрерывную величину (количество лет) в виде дискретной, ведь количество токенов в словаре модели ограничено.

То есть необходимо разбить все возможные значения количества лет на дискретные интервалы (провести дискретизацию).

Наивный подход к дискретизации --- разбить все возможные значения на интервалы, кажущиеся наиболее <<логичными>>, то есть 0-1 год, 1-3 года, 3-5 лет и так далее.

Однако был был выбран подход, учитывающий распределение количества лет во всех позициях всех резюме. Такой способ дискретизации позволяет исключить человеческий фактор в выборе интервалов и максимизировать <<полезность>> каждого токена.

Для того, чтобы провести дискретизацию, было сделано следующее:

\begin{itemize}
  \item Вычислена функция распределения количества проведённых лет на позиции.
  \item С помощью функции \texttt{KBinsDiscretizer} библиотеки \texttt{sklearn} \cite{pedregosa2018scikitlearnmachinelearningpython} были вычислены оптимальные границы для интервалов.
\end{itemize}

Распределение количества лет вместе в гистограммой, столбцы которой соответствуют дискретизированным интервалам, представлено на рисунке \ref{fig:experience_distribution}.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        height=0.6\textwidth,
        xlabel={Количество лет},
        ymin=0,
        xmin=0,
        xmax=30,
        ytick=\empty,
        ylabel=\empty,
        legend pos=north east
      ]
      \addplot+[
        ybar interval,
        fill=cyan,
        draw=black
      ] table {plots/histogram.dat};
      \addplot[
        red,
        thick
      ] table {plots/kde.dat};
      \legend{Дискретизированные интервалы, Плотность распределения}
    \end{axis}
  \end{tikzpicture}
  \caption{\centering Дискретизация токена, отражающего, сколько лет назад была окончена работа на некоторой позиции.}
  \label{fig:experience_distribution}
\end{figure}


В функцию \texttt{KBinsDiscretizer} было передано заранее определённое количество интервалов. Выбор количества интервалов сказывается на работе модели следующим образом:

\begin{itemize}
  \item Слишком большое количество интервалов приведёт к тому, что какой-либо интервал будет содержаться в слишком малом количестве обучающих примеров, и модель не сможет корректно выучить значение данного токена;
  \item Слишком малое количество интервалов приведёт к тому, что модель не будет видеть различие между значительной разницей в опыте.
\end{itemize}

Итоговый вид представления опыта кандидата с использованием специальных токенов представлен в таблице \ref{tab:position_encoding_example_2}.

\begin{table}[H]
  \centering
  \caption{\centering Пример составления строки для модели с использованием специальных токенов}
  \label{tab:position_encoding_example_2}
  \begin{tabular}{|c|p{7cm}|p{7cm}|}
    \hline
    \textbf{№} & \textbf{Псевдо-вакансия (anchor)}                   & \textbf{Опыт кандидата (positive)}                                                                                                                                                                                                                                                                                                                      \\
    \hline
    1          & IT Business Analyst <описание опыта на позиции>     & Junior Python Dev [2.0D] [2.0YA]\tablefootnote{[2.0D] --- токен <<2-4 года на позиции>> (D --- duration), [2.0YA] --- токен <<работа была завершена 2-4 года назад>> (YA --- years ago). Если позиция текущая, то вместо количества лет после окончания работы указывается 0.} [SEP] Python Backend Engineer [4.0D] [0.0YA] <описание опыта на позиции> \\
    \hline
    2          & Python Backend Engineer <описание опыта на позиции> & Junior Python Dev [2.0D] [0.0YA] <описание опыта на позиции>                                                                                                                                                                                                                                                                                            \\
    \hline
  \end{tabular}
\end{table}

Подход с добавлением токенов позволил модели лучше понимать, как давно и как долго кандидат работал на той или иной позиции, а значит, и оценивать его опытность более точно.


%=======================
\newpage
\section{Реализация приложения}\label{implementation}

Итоговое приложение было разработано с использованием библиотеки FastAPI и является набором API-методов, позволяющих рекрутинговой компании хранить вакансии и резюме, получать ранжированный список резюме для вакансии с помощью описанной в данной работе модели.

Архитектура приложения представлена на рисунке \ref{fig:system_architecture}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/system_architecture.pdf}
  \caption{Архитектура приложения}
  \label{fig:system_architecture}
\end{figure}

В качестве основной базы данных был использован PostgreSQL. Он содержит лишь неизменённые данные в формате JSONB, полученные в результате работы парсера.

Поиск и фильтрация же осуществляются в векторной базе данных qdrant.

qdrant был выбран из-за того, что скорость векторного поиска в нём значительно выше, чем в расширении, добавляющем векторный поиск в PostgreSQL (pgvector), а также из-за того, что в нём есть встроенный функционал для фильтрации векторов по метаданным.

Была исследована возможность проводить фильтрацию в PostgreSQL с последующим ранжированием отфильтрованных документов в qdrant, но скорость работы такого подхода оказалась неудовлетворительной.

Набор фильтров в qdrant менее разнообразен, чем в PostgreSQL, но в случае поставленной задачи является достаточным. Таким образом, qdrant отвечает и за ранжирование резюме, и за их фильтрацию.

Инференс deep learning модели осуществляется с помощью отдельного сервиса (ML-backend), который использует фреймворк Ray Serve для обработки запросов. Ray Serve является библиотекой для быстрого развёртывания и масштабирования моделей и обеспечивает возможность легкого добавления стандартных операций, требуемых для эффективной работы deep learning моделей, таких как батчинг и очередь запросов.



%=======================
\newpage
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}

В рамках данной работы была разработана система для поиска релевантных кандидатов по вакансиям с использованием методов машинного обучения. Проведен анализ специфики данных в сфере рекрутинга, выявлены особенности представления резюме и вакансий, которые необходимо учитывать при разработке системы поиска.

Проанализированы существующие подходы к семантическому поиску и ранжированию текстовых данных. На основе анализа был выбран подход с использованием эмбеддинговых моделей, как наиболее подходящий для решения поставленной задачи. Особое внимание было уделено проблеме асимметричности отношений между позициями в резюме и вакансиях.

Разработана и реализована эмбеддинговая модель для семантического поиска кандидатов. Для обучения модели был применен self-supervised подход, использующий карьерные переходы кандидатов для формирования обучающей выборки. Разработан метод кодирования опыта кандидата с помощью специальных токенов, что позволило модели лучше понимать временные характеристики опыта работы.

Создана система фильтрации и ранжирования резюме с учётом опыта кандидатов. Система позволяет эффективно отфильтровывать кандидатов по жёстким критериям и ранжировать оставшихся кандидатов с учётом их опыта работы.

Разработано и реализовано API для работы с системой поиска кандидатов. Система построена с использованием FastAPI и включает в себя PostgreSQL для хранения данных и qdrant для векторного поиска. Инференс deep learning модели осуществляется с помощью отдельного сервиса на базе Ray Serve.

Система успешно решает задачу поиска релевантных кандидатов, учитывая как семантическую близость резюме и вакансий, так и асимметричность отношений между позициями.

В качестве направлений для дальнейшего улучшения системы можно выделить:
\begin{itemize}
  \item исследование возможности использования более сложных методов фильтрации, учитывающих нечёткие критерии;
  \item улучшение лосс-функциии модели асимметричной оценки для повышения эффективности её обучения;
  \item расширение функциональности API для поддержки дополнительных сценариев использования.
\end{itemize}



%=======================
\newpage
\phantomsection
\addcontentsline{toc}{section}{Литература}
\renewcommand{\refname}{\centering \textbf{Литература}}

\bibliographystyle{rusnat}
\bibliography{references}

%=======================
\newpage
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}

\begin{lstlisting}[language=Python, caption={Реализация PairwiseTrainer для обучения модели асимметричной оценки}, tabsize=2, label=app-lst:1, basicstyle=\small\sffamily]
import torch
from transformers import Trainer

MARGIN = 0.01

class PairwiseTrainer(Trainer):
  def get_logits(self, model, inputs):
    input_junior = {
      'input_ids': (
        inputs['input_ids'][:, 0, :].squeeze(dim=1)
      ),
      'attention_mask': (
        inputs['attention_mask'][:, 0, :].squeeze(dim=1)
      ),
    }

    input_senior = {
      'input_ids': (
        inputs['input_ids'][:, 1, :].squeeze(dim=1)
      ),
      'attention_mask': (
        inputs['attention_mask'][:, 1, :].squeeze(dim=1)
      ),
    }
    
    output_junior = model(**input_junior)
    output_senior = model(**input_senior)

    return output_junior, output_senior
  
  def loss_from_logits(self, model,output_junior,  output_senior):
    diff = MARGIN + output_junior - output_senior
    
    junior_loss = model.distance_loss(
      output_junior, 
      torch.zeros_like(output_junior)
    )
    senior_loss = model.distance_loss(
      output_senior,
      torch.ones_like(output_senior)
    )
    
    seniority_loss = torch.where(
      diff >= 0,
      junior_loss + senior_loss,
      diff * 0
    )

    return torch.mean(seniority_loss)

  def compute_loss(self, model, inputs):
    output_junior, output_senior = self.get_logits(
      model, inputs
    )
    return self.loss_from_logits(
      model, output_junior, output_senior
    )
  
  def prediction_step(self, 
    model, 
    inputs, 
    prediction_loss_only, 
    ignore_keys=None
  ):
    with torch.no_grad():
      output_junior, output_senior = self.get_logits(
        model, inputs
        )
      loss = self.loss_from_logits(
        model, output_junior, output_senior
      )
      logits = torch.stack(
        (output_junior, output_senior), 
        dim=1
      ).detach()

      return (loss, logits, torch.ones_like(logits))
\end{lstlisting}


\end{document}
% ----------------------------------------------------------------


\lstset{ %
  language=C++,                 % выбор языка для подсветки (здесь это С++)
  basicstyle=\small\sffamily, % размер и начертание шрифта для подсветки кода
  numbers=left,               % где поставить нумерацию строк (слева\справа)
  numberstyle=\tiny,           % размер шрифта для номеров строк
  stepnumber=1,                   % размер шага между двумя номерами строк
  numbersep=5pt,                % как далеко отстоят номера строк от подсвечиваемого кода
  backgroundcolor=\color{white}, % цвет фона подсветки - используем \usepackage{color}
  showspaces=false,            % показывать или нет пробелы специальными отступами
  showstringspaces=false,      % показывать или нет пробелы в строках
  showtabs=false,             % показывать или нет табуляцию в строках
  frame=single,              % рисовать рамку вокруг кода
  tabsize=2,                 % размер табуляции по умолчанию равен 2 пробелам
  captionpos=t,              % позиция заголовка вверху [t] или внизу [b]
  breaklines=true,           % автоматически переносить строки (да\нет)
  breakatwhitespace=false, % переносить строки только если есть пробел
  escapeinside={\%*}{*)}   % если нужно добавить комментарии в коде
  extendedchars=true,
  commentstyle=\color{mygreen},    % comment style
  stringstyle=\bf,
  commentstyle=\ttfamily\itshape,
  keepspaces=true % пробелы между русскими буквами
  aboveskip=3mm,
  belowskip=3mm

}


\renewcommand\NAT@bibsetnum[1]{\settowidth\labelwidth{\@biblabel{#1}}%
  \setlength{\leftmargin}{\bibindent}\addtolength{\leftmargin}{\dimexpr\labelwidth+\labelsep\relax}%
  \setlength{\itemindent}{-\bibindent+\fivecharsapprox}%
  \setlength{\listparindent}{\itemindent}
  \setlength{\itemsep}{\bibsep}\setlength{\parsep}{\z@}%
  \ifNAT@openbib
    \addtolength{\leftmargin}{\bibindent}%
    \setlength{\itemindent}{-\bibindent}%
    \setlength{\listparindent}{\itemindent}%
    \setlength{\parsep}{0pt}%
  \fi
}
