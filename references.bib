@inproceedings{zhang-etal-2022-skillspan,
  title     = {{S}kill{S}pan: Hard and Soft Skill Extraction from {E}nglish Job Postings},
  author    = {Zhang, Mike  and
               Jensen, Kristian N{\o}rgaard  and
               Sonniks, Sif  and
               Plank, Barbara},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jul,
  year      = {2022},
  address   = {Seattle, United States},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.naacl-main.366},
  pages     = {4962--4984},
  abstract  = {Skill Extraction (SE) is an important and widely-studied task useful to gain insights into labor market dynamics. However, there is a lacuna of datasets and annotation guidelines; available datasets are few and contain crowd-sourced labels on the span-level or labels from a predefined skill inventory. To address this gap, we introduce SKILLSPAN, a novel SE dataset consisting of 14.5K sentences and over 12.5K annotated spans. We release its respective guidelines created over three different sources annotated for hard and soft skills by domain experts. We introduce a BERT baseline (Devlin et al., 2019). To improve upon this baseline, we experiment with language models that are optimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous pre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et al., 2020), and multi-task learning (Caruana, 1997). Our results show that the domain-adapted models significantly outperform their non-adapted counterparts, and single-task outperforms multi-task learning.}
}

@thesis{yacenko2023,
  title  = {Автоматический анализ текстов вакансий и резюме с помощью глубоких нейронных сетей},
  author = {Яценко А. А.},
  year   = {2023},
  school = {Южный федеральный университет},
  note   = {Институт математики, механики и компьютерных наук им. И.И. Воровича, Кафедра алгебры и дискретной математики},
  url    = {https://hub.sfedu.ru/repository/material/801315751/}
}


@article{radford2018improving,
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  title  = {Improving Language Understanding by Generative Pre-Training},
  year   = {2018}
}

@misc{pedregosa2018scikitlearnmachinelearningpython,
  title         = {Scikit-learn: Machine Learning in Python},
  author        = {Fabian Pedregosa and Gaël Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Andreas Müller and Joel Nothman and Gilles Louppe and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Édouard Duchesnay},
  year          = {2018},
  eprint        = {1201.0490},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1201.0490}
}

@inproceedings{zhang-etal-2024-nnose,
  title     = {{NNOSE}: Nearest Neighbor Occupational Skill Extraction},
  author    = {Zhang, Mike  and
               Goot, Rob  and
               Kan, Min-Yen  and
               Plank, Barbara},
  editor    = {Graham, Yvette  and
               Purver, Matthew},
  booktitle = {Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = mar,
  year      = {2024},
  address   = {St. Julian{'}s, Malta},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.eacl-long.35},
  pages     = {589--608},
  abstract  = {The labor market is changing rapidly, prompting increased interest in the automatic extraction of occupational skills from text. With the advent of English benchmark job description datasets, there is a need for systems that handle their diversity well. We tackle the complexity in occupational skill datasets tasks{---}combining and leveraging multiple datasets for skill extraction, to identify rarely observed skills within a dataset, and overcoming the scarcity of skills across datasets. In particular, we investigate the retrieval-augmentation of language models, employing an external datastore for retrieving similar skills in a dataset-unifying manner. Our proposed method, \textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill \textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by retrieving neighboring skills from other datasets in the datastore. This improves skill extraction \textit{without} additional fine-tuning. Crucially, we observe a performance gain in predicting infrequent patterns, with substantial gains of up to 30{\%} span-F1 in cross-dataset settings.}
}

@misc{joveo_sentencebert_2022,
  author       = {Dan, Sanchari},
  title        = {Using SentenceBERT to Generate Job Embeddings for Applications at Joveo},
  howpublished = {Blog post on Joveo},
  year         = {2022},
  month        = {9},
  url          = {https://www.joveo.com/blog/using-sentencebert-to-generate-job-embeddings-for-applications-at-joveo/}
}


@misc{devlin2019bertpretrainingdeepbidirectional,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1810.04805}
}

@misc{reimers2019sentencebertsentenceembeddingsusing,
  title         = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author        = {Nils Reimers and Iryna Gurevych},
  year          = {2019},
  eprint        = {1908.10084},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1908.10084}
}

@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}